# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16GL-I14_w25aNcca2V77f5CaHqQa7xXg
"""


#!/usr/bin/env python3
import os
import numpy as np
from numpy import random
import scipy
import matplotlib
import mnist
import pickle
matplotlib.use('agg')
import matplotlib.pyplot as plt

import time
import tensorflow as tf
# from tensorflow.keras.layers import Dense,Flatten,BatchNormalization,Conv2D,MaxPool2D
# from tensorflow.keras.optimizers import SGD,Adam
import random
mnist = tf.keras.datasets.mnist

if tf.__version__ != "2.1.0":
	print("-" * 40)
	print("**** warning: this assignment is tested on TensorFlow 2.1.0 or newer.")
	print("****         if you run into issues, please check that you are updated to the latest stable TensorFlow version")
	print("-" * 40)
# Device configuration
device = tf.device('/gpu:0' if tf.test.is_gpu_available() else 'cpu')
### hyperparameter settings and other constants
batch_size = 128
num_classes = 10
epochs = 10
mnist_input_shape = (28, 28, 1)
d1 = 1024
d2 = 256
alpha = 0.1
beta = 0.9
alpha_adam = 0.001
rho1 = 0.99
rho2 = 0.999
### end hyperparameter settings


# load the MNIST dataset using TensorFlow/Keras
def load_MNIST_dataset():
    mnist = tf.keras.datasets.mnist
    (Xs_tr, Ys_tr), (Xs_te, Ys_te) = mnist.load_data()
    Xs_tr = Xs_tr / 255.0
    Xs_te = Xs_te / 255.0
    Xs_tr = Xs_tr.reshape(Xs_tr.shape[0], 28, 28, 1) # 28 rows, 28 columns, 1 channel
    Xs_te = Xs_te.reshape(Xs_te.shape[0], 28, 28, 1)
    return (Xs_tr, Ys_tr, Xs_te, Ys_te)


# evaluate a trained model on MNIST data, and print the usual output from TF
#
# Xs        examples to evaluate on
# Ys        labels to evaluate on
# model     trained model
#
# returns   tuple of (loss, accuracy)
def evaluate_model(Xs, Ys, model):
    (loss, accuracy) = model.evaluate(Xs, Ys)
    return (loss, accuracy)


# train a fully connected two-hidden-layer neural network on MNIST data using SGD, and print the usual output from TF
#
# Xs        training examples
# Ys        training labels
# d1        the size of the first layer
# d2        the size of the second layer
# alpha     step size parameter
# beta      momentum parameter (0.0 if no momentum)
# B         minibatch size
# epochs    number of epochs to run
#
# returns   a tuple of
#   model       the trained model (should be of type tensorflow.python.keras.engine.sequential.Sequential)
#   history     the history of training returned by model.fit (should be of type tensorflow.python.keras.callbacks.History)
def train_fully_connected_sgd(Xs, Ys, d1, d2, alpha, beta, B, epochs):
    # TODO students should implement this
    n,i,j,_ = Xs.shape
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Flatten(input_shape=(i,j,1)))
    model.add(tf.keras.layers.Dense(d1, activation=tf.nn.relu))
    model.add(tf.keras.layers.Dense(d2, activation=tf.nn.relu))
    model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))
    
    model.compile(tf.keras.optimizers.SGD(lr=alpha, momentum=beta),
                  loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])
    history = model.fit(Xs,Ys,batch_size=B,epochs=epochs,validation_split=0.1)

    return (model, history)
# train a fully connected two-hidden-layer neural network on MNIST data using Adam, and print the usual output from TF
#
# Xs        training examples
# Ys        training labels
# d1        the size of the first layer
# d2        the size of the second layer
# alpha     step size parameter
# rho1      first moment decay parameter
# rho2      second moment decay parameter
# B         minibatch size
# epochs    number of epochs to run
#
# returns   a tuple of
#   model       the trained model (should be of type tensorflow.python.keras.engine.sequential.Sequential)
#   history     the history of training returned by model.fit (should be of type tensorflow.python.keras.callbacks.History)
def train_fully_connected_adam(Xs, Ys, d1, d2, alpha, rho1, rho2, B, epochs):
    # TODO students should implement this
    n,i,j,_ = Xs.shape
    model =  tf.keras.models.Sequential()
    model.add(tf.keras.layers.Flatten(input_shape=(i, j,1)))
    model.add(tf.keras.layers.Dense(d1, activation=tf.nn.relu))
    model.add(tf.keras.layers.Dense(d2, activation=tf.nn.relu))
    model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))
    
    model.compile(tf.keras.optimizers.Adam(lr=alpha,beta_1=rho1,beta_2=rho2),
                  loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])
    history = model.fit(Xs,Ys,batch_size=B,epochs=epochs,validation_split=0.1)
    return (model, history)
# train a fully connected two-hidden-layer neural network with Batch Normalization on MNIST data using SGD, and print the usual output from TF
#
# Xs        training examples
# Ys        training labels
# d1        the size of the first layer
# d2        the size of the second layer
# alpha     step size parameter
# beta      momentum parameter (0.0 if no momentum)
# B         minibatch size
# epochs    number of epochs to run
#
# returns   a tuple of
#   model       the trained model (should be of type tensorflow.python.keras.engine.sequential.Sequential)
#   history     the history of training returned by model.fit (should be of type tensorflow.python.keras.callbacks.History)
def train_fully_connected_bn_sgd(Xs, Ys, d1, d2, alpha, beta, B, epochs):
    # TODO students should implement this
    n,i,j,_ = Xs.shape
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Flatten(input_shape=(i,j,1)))
    model.add(tf.keras.layers.Dense(d1, activation=tf.nn.relu))
    model.add(tf.keras.layers.BatchNormalization(momentum=beta))
    model.add(tf.keras.layers.Dense(d2, activation=tf.nn.relu))
    model.add(tf.keras.layers.BatchNormalization(momentum=beta))
    model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))
    
    model.compile(tf.keras.optimizers.SGD(lr=alpha, momentum=beta),
                  loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])
    history = model.fit(Xs,Ys,batch_size=B,epochs=epochs,validation_split=0.1)
    return (model, history)

# train a convolutional neural network on MNIST data using SGD, and print the usual output from TF
#
# Xs        training examples
# Ys        training labels
# alpha     step size parameter
# rho1      first moment decay parameter
# rho2      second moment decay parameter
# B         minibatch size
# epochs    number of epochs to run
#
# returns   a tuple of
#   model       the trained model (should be of type tensorflow.python.keras.engine.sequential.Sequential)
#   history     the history of training returned by model.fit (should be of type tensorflow.python.keras.callbacks.History)
def train_CNN_sgd(Xs, Ys, alpha, rho1, rho2, B, epochs):
    # TODO students should implement this
    n,i,j,_ = Xs.shape
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Conv2D(32,kernel_size=(5,5),activation=tf.nn.relu,input_shape=(i,j,1)))
    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))
    model.add(tf.keras.layers.Conv2D(64,kernel_size=(5,5),activation=tf.nn.relu))
    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))
    model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))

    model.compile(optimizer=tf.keras.optimizers.Adam(lr=alpha,beta_1=rho1,beta_2=rho2),
                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    history = model.fit(Xs,Ys,batch_size=B,epochs=epochs,validation_split=0.1)
    return (model, history)
if __name__ == "__main__":
    (Xs_tr, Ys_tr, Xs_te, Ys_te) = load_MNIST_dataset()
    # TODO students should add code to generate plots here
#%% P1.1
    time_alg = np.zeros(4)
    d1 = 1024
    d2 = 256
    alpha = 0.1
    beta = 0
    B = 128
    epochs = 10
    start = time.time()
    model = train_fully_connected_sgd(Xs_tr, Ys_tr, d1, d2, alpha, beta, B, epochs)
    end = time.time()
    time_alg[0] = end - start
    floss,faccuracy = evaluate_model(Xs_te, Ys_te, model[0])

    plt.figure(1)
    plt.plot(model[1].history['loss']);plt.plot(model[1].history['val_loss']);plt.plot([floss for i in range(epochs)])
    plt.title('loss of SGD');plt.xlabel('epochs');plt.ylabel('loss');plt.legend(['train', 'valid','test']);plt.savefig('loss_SGD');plt.close()
    
    plt.figure(2)
    plt.plot(model[1].history['accuracy']);plt.plot(model[1].history['val_accuracy']);plt.plot([faccuracy for i in range(epochs)])
    plt.title('accuracy of SGD');plt.xlabel('epochs');plt.ylabel('accuracy');plt.legend(['train', 'valid','test']);plt.savefig('acc_SGD');plt.close()

#%% P 1.2
    d1 = 1024
    d2 = 256
    alpha = 0.1
    beta = 0.9
    B = 128
    epochs = 10
    start = time.time()
    model = train_fully_connected_sgd(Xs_tr, Ys_tr, d1, d2, alpha, beta, B, epochs)
    end = time.time()
    time_alg[1] = end - start
    floss,faccuracy = evaluate_model(Xs_te, Ys_te, model[0])

    plt.figure(3)
    plt.plot(model[1].history['loss']);plt.plot(model[1].history['val_loss']);plt.plot([floss for i in range(epochs)])
    plt.title('loss of SGD_moment');plt.xlabel('epochs');plt.ylabel('loss');plt.legend(['train', 'valid','test']);plt.savefig('loss_SGDmoment');plt.close()
    plt.figure(4)
    plt.plot(model[1].history['accuracy']);plt.plot(model[1].history['val_accuracy']);plt.plot([faccuracy for i in range(epochs)])
    plt.title('accuracy of SGD_moment');plt.xlabel('epochs');plt.ylabel('accuracy');plt.legend(['train', 'valid','test']);plt.savefig('acc_SGDmoment');plt.close()

#%%"""P 1.3"""
    d1 = 1024
    d2 = 256
    alpha = 0.001
    rho1 = 0.99
    rho2 = 0.999
    B = 128
    epochs = 10
    start = time.time()
    model = train_fully_connected_adam(Xs_tr, Ys_tr, d1, d2, alpha, rho1, rho2, B, epochs)
    end = time.time()
    time_alg[2] = end - start
    floss,faccuracy = evaluate_model(Xs_te, Ys_te, model[0])

    plt.figure(5)
    plt.plot(model[1].history['loss']);plt.plot(model[1].history['val_loss']);plt.plot([floss for i in range(epochs)])
    plt.title('loss of Adam');plt.xlabel('epochs');plt.ylabel('loss');plt.legend(['train', 'valid','test']);plt.savefig('loss_Adam');plt.close()

    plt.figure(6)
    plt.plot(model[1].history['accuracy']);plt.plot(model[1].history['val_accuracy']);plt.plot([faccuracy for i in range(epochs)])
    plt.title('accuracy of Adam');plt.xlabel('epochs');plt.ylabel('accuracy');plt.legend(['train', 'valid','test']);plt.savefig('acc_Adam');plt.close()

#%%"""P 1.4"""

    d1 = 1024
    d2 = 256
    alpha = 0.001
    beta = 0.9
    B = 128
    epochs = 10
    start = time.time()
    model = train_fully_connected_bn_sgd(Xs_tr, Ys_tr, d1, d2, alpha, beta, B, epochs)
    end = time.time()
    time_alg[3] = end - start
    floss,faccuracy = evaluate_model(Xs_te, Ys_te, model[0])

    plt.figure(7)
    plt.plot(model[1].history['loss']);plt.plot(model[1].history['val_loss']);plt.plot([floss for i in range(epochs)])
    plt.title('loss of BN');plt.xlabel('epochs');plt.ylabel('loss');plt.legend(['train', 'valid','test']);plt.savefig('loss_bn');plt.close()
    plt.figure(8)
    plt.plot(model[1].history['accuracy']);plt.plot(model[1].history['val_accuracy']);plt.plot([faccuracy for i in range(epochs)])
    plt.title('accuracy of BN');plt.xlabel('epochs');plt.ylabel('accuracy');plt.legend(['train', 'valid','test']);plt.savefig('acc_bn');plt.close()

    plt.figure(9)
    number = [1,2,3,4]
    plt.plot(number, time_alg)
    plt.title('time of different alg');plt.xlabel('SGD   SGD_Moment   Adam   SGD_BN');plt.xlim(1, 4);plt.ylabel('second');plt.savefig('time_4');plt.close()
    print(time_alg)

#%%"""P 2-1 Grid Search"""

    d1 = 1024
    d2 = 256
    epochs = 10
    
    
    alpha_G = [0.1, 0.03, 0.01]
    beta_G = [0.9, 0.7, 0.5]
    B_G = [128, 64]
    for i in range(len(alpha_G)):
        alpha = alpha_G[i]
        for j in range(len(beta_G)):
            beta = beta_G[j]
            loss_valid_G = []
            acc_valid_G = []
            for k in range(len(B_G)):
                B = B_G[k]
                model = train_fully_connected_sgd(Xs_tr, Ys_tr, d1, d2, alpha, beta, B, epochs)
                floss,faccuracy = evaluate_model(Xs_te, Ys_te, model[0])
                temp1 = model[1].history['val_loss']
                temp2 = model[1].history['val_accuracy']
                loss_valid_G.append(temp1[-1])
                acc_valid_G.append(temp2[-1])
            print(loss_valid_G)
            print(acc_valid_G)

    alpha = 0.1
    beta = 0.5
    B = 64
    model = train_fully_connected_sgd(Xs_tr, Ys_tr, d1, d2, alpha, beta, B, epochs)
    floss,faccuracy = evaluate_model(Xs_te, Ys_te, model[0])
    print(floss)
    print(faccuracy)

#%%"""P2-3 Random search"""

    for i in range(18):
        print(i)
        alpha = random.randrange(1,11,1)/100
        print(alpha)
        beta = random.randrange(5,10,1)/10
        print(beta)
        loss_valid_G = []
        acc_valid_G = []
        B = random.randrange(64,129,64)
        print(B)
        model = train_fully_connected_sgd(Xs_tr, Ys_tr, d1, d2, alpha, beta, B, epochs)
        floss,faccuracy = evaluate_model(Xs_te, Ys_te, model[0])
        temp1 = model[1].history['val_loss']
        temp2 = model[1].history['val_accuracy']
        loss_valid_G.append(temp1[-1])
        acc_valid_G.append(temp2[-1])
        print(loss_valid_G)
        print(acc_valid_G)
#%% P3 trainning CNN
    alpha = 0.001
    rho1 = 0.99
    rho2 = 0.999
    B = 128
    epochs = 10
    start = time.time()
    print(Xs_tr.shape)

    start = time.time()
    model = train_CNN_sgd(Xs_tr, Ys_tr, alpha, rho1, rho2, B, epochs)
    end = time.time()
    floss,faccuracy = evaluate_model(Xs_te, Ys_te, model[0])

    plt.figure(12)
    plt.plot(model[1].history['loss']);plt.plot(model[1].history['val_loss']);plt.plot([floss for i in range(epochs)])
    plt.title('loss of CNN');plt.xlabel('epochs');plt.ylabel('loss');plt.legend(['train', 'valid','test']);plt.savefig('loss_CNN');plt.close()
    plt.figure(13)
    plt.plot(model[1].history['accuracy']);plt.plot(model[1].history['val_accuracy']);plt.plot([faccuracy for i in range(epochs)])
    plt.title('accuracy of CNN');plt.xlabel('epochs');plt.ylabel('accuracy');plt.legend(['train', 'valid','test']);plt.savefig('acc_CNN');plt.close()
    print(end-start)